# Set up logging configuration
import logging
logging.basicConfig(filename='simplifiedFL.log', level=logging.INFO, format='%(message)s')
import tensorflow as tf

# Check if TensorFlow is able to detect the GPU
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    try:
        # Currently, memory growth needs to be the same across GPUs
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
        logical_gpus = tf.config.experimental.list_logical_devices('GPU')
        print(len(gpus), "Physical GPUs,", len(logical_gpus), "Logical GPUs")
    except RuntimeError as e:
        # Memory growth must be set before GPUs have been initialized
        print(e)
else:
    print("No GPU detected. Running on CPU.")

import numpy as np
from tensorflow.keras.datasets import cifar10
from tensorflow.keras.utils import to_categorical
from tensorflow.keras import layers, models
from sklearn.model_selection import train_test_split
from tensorflow.keras.optimizers import Adam

# Set random seeds for reproducibility
np.random.seed(5)
tf.random.set_seed(5)

def train_validation_split(X_train, Y_train):
    train_length = len(X_train)
    validation_length = int(train_length / 4)
    X_validation = X_train[:validation_length]
    X_train = X_train[validation_length:]
    Y_validation = Y_train[:validation_length]
    Y_train = Y_train[validation_length:]
    return X_train, Y_train, X_validation, Y_validation

# Network settings
learning_rate = 0.01
epochs = 1
batch = 32
iterations = 10
number_of_users = 20

# Load and preprocess CIFAR-10 dataset
(X_train, Y_train), (X_test, Y_test) = cifar10.load_data()
X_train, X_test = X_train / 255.0, X_test / 255.0
Y_train, Y_test = to_categorical(Y_train), to_categorical(Y_test)

# FL setting
size_of_user_ds = int(len(X_train)/number_of_users)
train_data_X = np.zeros((number_of_users,size_of_user_ds, 32, 32, 3))
train_data_Y = np.ones((number_of_users,size_of_user_ds,10))

for i in range(number_of_users):
    train_data_X[i] = X_train[size_of_user_ds*i:size_of_user_ds*i+size_of_user_ds]
    train_data_Y[i] = Y_train[size_of_user_ds*i:size_of_user_ds*i+size_of_user_ds]



# Model definition
initializer = tf.keras.initializers.HeUniform(seed=5)
model = models.Sequential([
    # Add layers as per the provided structure
  layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer=tf.keras.initializers.HeUniform(seed=5), padding='same', input_shape=(32, 32, 3)),
  layers.BatchNormalization(),
  layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer=tf.keras.initializers.HeUniform(seed=5), padding='same'),
  layers.BatchNormalization(),
  layers.MaxPooling2D((2, 2)),
  layers.Dropout(0.2),
  layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer=tf.keras.initializers.HeUniform(seed=5), padding='same'),
  layers.BatchNormalization(),
  layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer=tf.keras.initializers.HeUniform(seed=5), padding='same'),
  layers.BatchNormalization(),
  layers.MaxPooling2D((2, 2)),
  layers.Dropout(0.3),
  layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer=tf.keras.initializers.HeUniform(seed=5), padding='same'),
  layers.BatchNormalization(),
  layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer=tf.keras.initializers.HeUniform(seed=5), padding='same'),
  layers.BatchNormalization(),
  layers.MaxPooling2D((2, 2)),
  layers.Dropout(0.4),
  layers.Flatten(),
  layers.Dense(128, activation='relu', kernel_initializer=tf.keras.initializers.HeUniform(seed=5)),
  layers.BatchNormalization(),
  layers.Dropout(0.5),
  layers.Dense(10, activation='softmax'),
])

model_name = model._name
opt = Adam(learning_rate=0.0001) if model_name not in ['VGG16', 'resnet18'] else Adam()
model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])

# Federated Learning Process
wc = model.get_weights()  # Initial global model weights
logging.info(f"\n Simulating with number_of_users: {number_of_users}")
for iter in range(iterations):
    gradients = []
    total_data_points = 0

    for i in range(number_of_users):
        X_train_u = train_data_X[i]
        Y_train_u = train_data_Y[i]

        # Shuffle the data
        np.random.seed(5)
        shuffler = np.random.permutation(len(X_train_u))
        X_train_u = X_train_u[shuffler]
        Y_train_u = Y_train_u[shuffler]
        # Split the data into training and validation sets
        X_train_u, Y_train_u, X_val_u, Y_val_u = train_validation_split(X_train_u, Y_train_u)

        # Train model for each user
        model.set_weights(wc)
        model.fit(X_train_u, Y_train_u, epochs=epochs, batch_size=batch, validation_data=(X_val_u, Y_val_u), shuffle=False)

        # Gradient calculation
        wu = model.get_weights()
        nu = len(X_train_u) + len(X_val_u)
        total_data_points += nu
        gradient = [wu[j] - wc[j] for j in range(len(wc))]
        weighted_gradient = [g * nu for g in gradient]
        gradients.append(weighted_gradient)

    # Aggregate gradients
    new_weights = [np.sum([gradients[user][i] for user in range(number_of_users)], axis=0) / total_data_points for i in range(len(wc))]
    wc = [wc[i] + new_weights[i] for i in range(len(wc))]

    # Update global model
    model.set_weights(wc)

    # Evaluate global model
    _, accuracy = model.evaluate(X_test, Y_test)
    logging.info(f"Iteration {iter + 1}, Accuracy: {accuracy:.4f}")

# Save final model
model.save("federated_model2users.h5")

